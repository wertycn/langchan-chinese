# C Transformers

本页面介绍如何在LangChain中使用[C Transformers](https://github.com/marella/ctransformers)库。
它分为两个部分：安装和设置，以及对特定C Transformers包装器的引用。

## 安装和设置

- 使用`pip install ctransformers`命令安装Python包
- 下载一个支持的[GGML模型](https://huggingface.co/TheBloke)（参见[支持的模型](https://github.com/marella/ctransformers#supported-models)）

## 包装器

### LLM

存在一个CTransformers LLM包装器，您可以通过以下方式访问:

```python
from langchain.llms import CTransformers
```

它为所有模型提供了统一的接口：

```python
llm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2')

print(llm('AI is going to'))
```

如果您遇到“非法指令”错误，请尝试使用`lib='avx'`或`lib='basic'`：

```py
llm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2', lib='avx')
```

它可以与托管在Hugging Face Hub上的模型一起使用:

```py
llm = CTransformers(model='marella/gpt-2-ggml')
```

如果一个模型仓库有多个模型文件（`.bin`文件），可以使用以下方式指定一个模型文件：

```py
llm = CTransformers(model='marella/gpt-2-ggml', model_file='ggml-model.bin')
```

可以使用`config`参数传递额外的参数：

```py
config = {'max_new_tokens': 256, 'repetition_penalty': 1.1}

llm = CTransformers(model='marella/gpt-2-ggml', config=config)
```

请参阅[文档](https://github.com/marella/ctransformers#config)获取可用参数列表。

有关更详细的步骤说明，请参阅[此笔记本](/docs/modules/model_io/models/llms/integrations/ctransformers.html)。
