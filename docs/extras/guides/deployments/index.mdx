# 部署

在当今快节奏的技术环境中，大型语言模型（LLM）的使用正在迅速扩大。因此，开发人员需要了解如何在生产环境中有效地部署这些模型。LLM接口通常分为两类：

- **情况1：利用外部LLM提供商（OpenAI，Anthropic等）**
    在这种情况下，大部分的计算负担由LLM提供商承担，而LangChain简化了围绕这些服务实现业务逻辑的过程。这种方法包括诸如提示模板化、聊天消息生成、缓存、向量嵌入数据库创建、预处理等功能。

- **案例2: 自托管的开源模型**
    或者，开发人员可以选择使用更小但功能相当的自托管开源LLM模型。这种方法可以显著降低将数据传输给外部LLM提供商所带来的成本、延迟和隐私问题。

无论你的产品的基础框架是什么，部署LLM应用程序都会带来一系列的挑战。在评估服务框架时，了解权衡和关键考虑因素非常重要。

## 大纲

本指南旨在全面介绍在生产环境中部署LLM（语言模型）所需的要求，重点关注以下几个方面：

- **设计稳健的LLM应用服务**
- **保持成本效益**
- **确保快速迭代**

了解这些组件在评估服务系统时非常重要。LangChain与几个开源项目集成，旨在解决这些问题，并为您的LLM应用程序提供一个强大的框架。一些值得注意的框架包括：

- [Ray Serve](/docs/ecosystem/integrations/ray_serve.html)
- [BentoML](https://github.com/ssheng/BentoChain)
- [Modal](/docs/ecosystem/integrations/modal.html)

这些链接将提供有关每个生态系统的更多信息，帮助您找到最适合您的LLM部署需求的解决方案。

## 设计稳健的LLM应用服务

在将LLM服务部署到生产环境中时，提供一个没有中断的无缝用户体验非常重要。实现全天候的服务可用性涉及创建和维护围绕您的应用程序的多个子系统。

### 监控

监控是在生产环境中运行的任何系统的重要组成部分。在LLMs的背景下，监控性能和质量指标至关重要。

**性能指标：**这些指标提供了有关模型效率和容量的见解。以下是一些关键示例：

- 每秒查询数（QPS）：这衡量了模型每秒处理的查询数量，提供了有关其利用率的见解。
- 延迟（Latency）：该指标衡量从客户端发送请求到接收到响应的延迟时间。
- 每秒生成的令牌数（TPS）：表示模型每秒可以生成的令牌数量。

**质量指标：** 这些指标通常根据业务用例进行定制。例如，您的系统输出与基准（如先前版本）相比如何？尽管可以离线计算这些指标，但您需要记录必要的数据以供以后使用。

### 容错性

您的应用程序可能会遇到一些错误，比如模型推理或业务逻辑代码中的异常，导致失败并中断流量。其他潜在问题可能来自运行应用程序的机器，例如在高需求时意外的硬件故障或丢失的spot-instances。减轻这些风险的一种方法是通过副本扩展和为失败的副本实施恢复机制来增加冗余。然而，模型副本并不是唯一可能出现故障的地方。建立针对在堆栈的任何位置可能发生的各种故障的弹性是至关重要的。


### 零停机升级

系统升级通常是必要的，但如果处理不当可能会导致服务中断。防止升级期间停机的一种方法是实施平滑过渡过程，从旧版本过渡到新版本。理想情况下，您的LLM服务的新版本已部署，并且流量逐渐从旧版本转移到新版本，在整个过程中保持恒定的QPS。

### 负载均衡

负载均衡，简单来说，是一种将工作均匀分配到多台计算机、服务器或其他资源上的技术，以优化系统的利用率，最大化吞吐量，最小化响应时间，并避免任何单个资源的过载。可以将其想象为交通警察将汽车（请求）引导到不同的道路（服务器），以确保没有任何一条道路过于拥堵。

有几种负载均衡的策略。例如，一种常见的方法是*轮询*策略，每个请求按顺序发送到下一个服务器，当所有服务器都收到请求后，循环回到第一个服务器。当所有服务器能力相等时，这种方法效果很好。然而，如果某些服务器比其他服务器更强大，您可以使用*加权轮询*或*最少连接*策略，将更多请求发送到更强大的服务器，或者发送到当前处理最少活动请求的服务器。让我们想象一下，您正在运行一个LLM链。如果您的应用程序变得流行起来，可能会有数百甚至数千个用户同时提问。如果一个服务器变得太忙（负载高），负载均衡器会将新的请求定向到另一个负载较轻的服务器。这样，所有用户都能及时获得响应，系统保持稳定。



## 保持成本效益和可扩展性

部署LLM服务可能会很昂贵，特别是当您处理大量用户交互时。LLM提供商通常按照使用的标记计费，这意味着在这些模型上进行聊天系统推理可能会很昂贵。然而，有几种策略可以帮助管理这些成本，而不会影响服务的质量。

### 自托管模型

现在有一些较小的开源LLM（语言模型）正在出现，以解决对LLM提供商的依赖问题。自主托管可以让您在控制成本的同时保持类似LLM提供商模型的质量。挑战在于在自己的机器上构建一个可靠、高性能的LLM服务系统。

### 资源管理和自动扩展

在您的应用程序中，计算逻辑需要精确的资源分配。例如，如果您的一部分流量由OpenAI端点提供，另一部分由自托管模型提供，为每个部分分配适当的资源非常重要。根据流量自动调整资源分配（自动扩展）可以显著影响运行应用程序的成本。这种策略需要在成本和响应性之间保持平衡，确保既不过度提供资源，也不影响应用程序的响应能力。

### 利用竞价实例

在像AWS这样的平台上，竞价实例提供了可观的成本节省，通常价格约为按需实例的三分之一。不过，这种折衷是更高的崩溃率，需要一个强大的容错机制来实现有效的使用。

### 独立扩展

当自行托管模型时，您应该考虑独立缩放。例如，如果您有两个翻译模型，一个针对法语进行了微调，另一个针对西班牙语进行了微调，那么传入的请求可能需要针对每个模型进行不同的缩放要求。

### 批处理请求

在大型语言模型的背景下，将请求进行批处理可以通过更好地利用GPU资源来提高效率。GPU是并行处理器，设计用于同时处理多个任务。如果您将单独的请求发送到模型，GPU可能无法充分利用，因为它一次只处理一个任务。另一方面，通过将请求进行批处理，您可以让GPU同时处理多个任务，最大限度地利用它的资源，并提高推理速度。这不仅可以节省成本，还可以提高LLM服务的整体延迟。


总结起来，要在扩展LLM服务的同时管理成本，需要采取战略性的方法。利用自托管模型、有效管理资源、使用自动扩展、使用竞价实例、独立扩展模型和批量处理请求是需要考虑的关键策略。开源库如Ray Serve和BentoML旨在处理这些复杂性。

## 确保迅速迭代

LLM（语言模型）领域正在以前所未有的速度发展，不断引入新的库和模型架构。因此，避免将自己局限于特定框架的解决方案至关重要。这在模型服务方面尤为重要，因为对基础架构进行更改可能耗时、昂贵且具有风险。努力构建一个不依赖于任何特定机器学习库或框架的基础架构，而是提供一个通用的、可扩展的服务层。以下是一些灵活性发挥关键作用的方面：

### 模型组合

部署像LangChain这样的系统需要能够将不同的模型组合在一起，并通过逻辑连接它们。以构建一个自然语言输入SQL查询引擎为例。查询LLM并获取SQL命令只是系统的一部分。您需要从连接的数据库中提取元数据，为LLM构建提示，对引擎运行SQL查询，在查询运行时收集和反馈响应给LLM，并将结果呈现给用户。这说明了将Python中构建的各种复杂组件无缝集成到一起的需求，形成一个动态的逻辑块链，可以一起提供服务。

## 云服务提供商

许多托管解决方案仅限于单个云服务提供商，这可能会限制您在当今多云世界中的选择。根据您的其他基础架构组件构建的位置，您可能更喜欢坚持选择的云服务提供商。


## 基础架构即代码 (IaC)

快速迭代还涉及到能够快速、可靠地重新创建基础设施的能力。这就是基础设施即代码（IaC）工具如 Terraform、CloudFormation 或 Kubernetes YAML 文件的作用所在。它们允许您在代码文件中定义基础设施，并且这些文件可以进行版本控制和快速部署，从而实现更快速、更可靠的迭代。

## CI/CD

在快节奏的环境中，实施CI/CD流水线可以显著加快迭代过程。它们帮助自动化LLM应用程序的测试和部署，降低错误风险，实现更快的反馈和迭代。