以下是最简单的示例：

```python
from langchain import PromptTemplate


template = """/
You are a naming consultant for new companies.
What is a good name for a company that makes {product}?
"""

prompt = PromptTemplate.from_template(template)
prompt.format(product="colorful socks")
```

<CodeOutputBlock lang="python">

```
    I want you to act as a naming consultant for new companies.
    What is a good name for a company that makes colorful socks?
```

</CodeOutputBlock>


## 创建提示模板

您可以使用`PromptTemplate`类创建简单的硬编码提示。提示模板可以接受任意数量的输入变量，并且可以格式化生成提示。


```python
from langchain import PromptTemplate

# An example prompt with no input variables
no_input_prompt = PromptTemplate(input_variables=[], template="Tell me a joke.")
no_input_prompt.format()
# -> "Tell me a joke."

# An example prompt with one input variable
one_input_prompt = PromptTemplate(input_variables=["adjective"], template="Tell me a {adjective} joke.")
one_input_prompt.format(adjective="funny")
# -> "Tell me a funny joke."

# An example prompt with multiple input variables
multiple_input_prompt = PromptTemplate(
    input_variables=["adjective", "content"], 
    template="Tell me a {adjective} joke about {content}."
)
multiple_input_prompt.format(adjective="funny", content="chickens")
# -> "Tell me a funny joke about chickens."
```

如果您不希望手动指定`input_variables`，还可以使用`from_template`类方法创建一个`PromptTemplate`。`langchain`将根据传递的`template`自动推断出`input_variables`。

```python
template = "Tell me a {adjective} joke about {content}."

prompt_template = PromptTemplate.from_template(template)
prompt_template.input_variables
# -> ['adjective', 'content']
prompt_template.format(adjective="funny", content="chickens")
# -> Tell me a funny joke about chickens.
```

您可以创建自定义的提示模板，以任何您想要的方式格式化提示。更多信息，请参阅[自定义提示模板](./custom_prompt_template.html)。

<!-- TODO（shreya）：添加到Jinja的链接 -->

## 聊天提示模板

[聊天模型](../models/chat)以一个`聊天消息列表`作为输入 - 这个列表通常被称为`提示`。
这些聊天消息与原始字符串（您将传递给[LLM](/docs/modules/model_io/models/llms)模型的字符串）不同，因为每条消息都与一个“角色”相关联。

例如，在OpenAI的[Chat Completion API](https://platform.openai.com/docs/guides/chat/introduction)中，聊天消息可以与AI、人类或系统角色相关联。模型应更加密切地遵循系统聊天消息的指示。

LangChain提供了多种提示模板，使构建和处理提示变得更加容易。建议您在查询聊天模型时使用这些与聊天相关的提示模板，而不是使用`PromptTemplate`，以充分发挥底层聊天模型的潜力。

<!-- 警告：此文件是自动生成的！请勿编辑！相反，请编辑具有位置和名称的笔记本文件。 -->


```python
from langchain.prompts import (
    ChatPromptTemplate,
    PromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)
```

要创建与角色关联的消息模板，您可以使用`MessagePromptTemplate`。

为了方便起见，模板上暴露了一个`from_template`方法。如果您使用这个模板，它的样子会是这样的：


```python
template="You are a helpful assistant that translates {input_language} to {output_language}."
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
human_template="{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
```

如果您想更直接地构建`MessagePromptTemplate`，您可以在外部创建一个PromptTemplate，然后将其传递进去，例如：


```python
prompt=PromptTemplate(
    template="You are a helpful assistant that translates {input_language} to {output_language}.",
    input_variables=["input_language", "output_language"],
)
system_message_prompt_2 = SystemMessagePromptTemplate(prompt=prompt)

assert system_message_prompt == system_message_prompt_2
```

之后，您可以根据一个或多个`MessagePromptTemplates`构建一个`ChatPromptTemplate`。您可以使用`ChatPromptTemplate`的`format_prompt`方法，该方法返回一个`PromptValue`。您可以将其转换为字符串或消息对象，具体取决于您是否希望将格式化后的值作为输入传递给llm或chat模型。


```python
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# get a chat completion from the formatted messages
chat_prompt.format_prompt(input_language="English", output_language="French", text="I love programming.").to_messages()
```

<CodeOutputBlock lang="python">

```
    [SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}),
     HumanMessage(content='I love programming.', additional_kwargs={})]
```

</CodeOutputBlock>
