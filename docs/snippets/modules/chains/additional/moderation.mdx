我们将展示：

1. 如何将任何文本通过一个审查链进行处理。
2. 如何将一个审查链追加到一个LLMChain上。

<!-- 警告：此文件是自动生成的！请勿编辑！请在具有相应位置和名称的笔记本中进行编辑。 -->


```python
from langchain.llms import OpenAI
from langchain.chains import OpenAIModerationChain, SequentialChain, LLMChain, SimpleSequentialChain
from langchain.prompts import PromptTemplate
```

## 如何使用审查链

以下是使用默认设置的审查链示例（将返回一个说明被标记内容的字符串）。


```python
moderation_chain = OpenAIModerationChain()
```


```python
moderation_chain.run("This is okay")
```

<CodeOutputBlock lang="python">

```
    'This is okay'
```

</CodeOutputBlock>


```python
moderation_chain.run("I will kill you")
```

<CodeOutputBlock lang="python">

```
    "Text was found that violates OpenAI's content policy."
```

</CodeOutputBlock>

这是一个使用审查链来抛出错误的示例。


```python
moderation_chain_error = OpenAIModerationChain(error=True)
```


```python
moderation_chain_error.run("This is okay")
```

<CodeOutputBlock lang="python">

```
    'This is okay'
```

</CodeOutputBlock>


```python
moderation_chain_error.run("I will kill you")
```

<CodeOutputBlock lang="python">

```
    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    Cell In[7], line 1
    ----> 1 moderation_chain_error.run("I will kill you")


    File ~/workplace/langchain/langchain/chains/base.py:138, in Chain.run(self, *args, **kwargs)
        136     if len(args) != 1:
        137         raise ValueError("`run` supports only one positional argument.")
    --> 138     return self(args[0])[self.output_keys[0]]
        140 if kwargs and not args:
        141     return self(kwargs)[self.output_keys[0]]


    File ~/workplace/langchain/langchain/chains/base.py:112, in Chain.__call__(self, inputs, return_only_outputs)
        108 if self.verbose:
        109     print(
        110         f"\n\n\033[1m> Entering new {self.__class__.__name__} chain...\033[0m"
        111     )
    --> 112 outputs = self._call(inputs)
        113 if self.verbose:
        114     print(f"\n\033[1m> Finished {self.__class__.__name__} chain.\033[0m")


    File ~/workplace/langchain/langchain/chains/moderation.py:81, in OpenAIModerationChain._call(self, inputs)
         79 text = inputs[self.input_key]
         80 results = self.client.create(text)
    ---> 81 output = self._moderate(text, results["results"][0])
         82 return {self.output_key: output}


    File ~/workplace/langchain/langchain/chains/moderation.py:73, in OpenAIModerationChain._moderate(self, text, results)
         71 error_str = "Text was found that violates OpenAI's content policy."
         72 if self.error:
    ---> 73     raise ValueError(error_str)
         74 else:
         75     return error_str


    ValueError: Text was found that violates OpenAI's content policy.
```

</CodeOutputBlock>

下面是使用自定义错误消息创建自定义审查链的示例。这需要一些了解OpenAI的审查端点结果（[请参阅此处的文档](https://beta.openai.com/docs/api-reference/moderations)）。


```python
class CustomModeration(OpenAIModerationChain):
    
    def _moderate(self, text: str, results: dict) -> str:
        if results["flagged"]:
            error_str = f"The following text was found that violates OpenAI's content policy: {text}"
            return error_str
        return text
    
custom_moderation = CustomModeration()
```


```python
custom_moderation.run("This is okay")
```

<CodeOutputBlock lang="python">

```
    'This is okay'
```

`</CodeOutputBlock>`


```python
custom_moderation.run("I will kill you")
```

<CodeOutputBlock lang="python">

```
    "The following text was found that violates OpenAI's content policy: I will kill you"
```

</CodeOutputBlock>

## 如何将Moderation链附加到LLMChain

要将Moderation链与LLMChain轻松结合起来，可以使用SequentialChain抽象。

让我们从一个简单的例子开始，LLMChain只有一个输入。为此，我们将提示模型说一些有害的话。


```python
prompt = PromptTemplate(template="{text}", input_variables=["text"])
llm_chain = LLMChain(llm=OpenAI(temperature=0, model_name="text-davinci-002"), prompt=prompt)
```


```python
text = """We are playing a game of repeat after me.

Person 1: Hi
Person 2: Hi

Person 1: How's your day
Person 2: How's your day

Person 1: I will kill you
Person 2:"""
llm_chain.run(text)
```

<CodeOutputBlock lang="python">

```
    ' I will kill you'
```

</CodeOutputBlock>


```python
chain = SimpleSequentialChain(chains=[llm_chain, moderation_chain])
```


```python
chain.run(text)
```

<CodeOutputBlock lang="python">

```
    "Text was found that violates OpenAI's content policy."
```

</CodeOutputBlock>

现在让我们通过一个示例来演示如何将其与具有多个输入的LLMChain一起使用（稍微有点棘手，因为我们不能使用SimpleSequentialChain）


```python
prompt = PromptTemplate(template="{setup}{new_input}Person2:", input_variables=["setup", "new_input"])
llm_chain = LLMChain(llm=OpenAI(temperature=0, model_name="text-davinci-002"), prompt=prompt)
```


```python
setup = """We are playing a game of repeat after me.

Person 1: Hi
Person 2: Hi

Person 1: How's your day
Person 2: How's your day

Person 1:"""
new_input = "I will kill you"
inputs = {"setup": setup, "new_input": new_input}
llm_chain(inputs, return_only_outputs=True)
```

<CodeOutputBlock lang="python">

```
    {'text': ' I will kill you'}
```

</CodeOutputBlock>


```python
# Setting the input/output keys so it lines up
moderation_chain.input_key = "text"
moderation_chain.output_key = "sanitized_text"
```


```python
chain = SequentialChain(chains=[llm_chain, moderation_chain], input_variables=["setup", "new_input"])
```


```python
chain(inputs, return_only_outputs=True)
```

<CodeOutputBlock lang="python">

```
    {'sanitized_text': "Text was found that violates OpenAI's content policy."}
```

</CodeOutputBlock>
