# 内容审核
本笔记本演示了如何使用一个审查链以及几种常见的方法。审查链对于检测可能包含令人讨厌、暴力等内容的文本非常有用。这对用户输入以及语言模型的输出都非常有用。一些API提供者，比如OpenAI，[明确禁止](https://beta.openai.com/docs/usage-policies/use-case-policy)您或您的最终用户生成某些类型的有害内容。为了遵守这一规定（以及防止您的应用程序具有有害性），您通常希望在任何LLMChains后面添加一个审查链，以确保LLM生成的任何输出都不会有害。

如果传入审查链的内容有害，处理它的最佳方式可能因您的应用程序而异。有时您可能希望在链中抛出错误（并让应用程序处理该错误）。其他时候，您可能希望向用户返回一些解释，说明文本是有害的。甚至可能有其他处理方式！我们将在本教程中涵盖所有这些方式。

从 "@snippets/modules/chains/additional/moderation.mdx" 导入 Example

<Example/>
